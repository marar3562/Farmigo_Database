---
title: "Process Anonymized Files to Output Raw Dataset"
output: html_notebook
---


## Libraries

```{r}
library(tidyverse)
library(lubridate)
library(googlesheets4)
```

## Folder Structure

```{r}
start_time = Sys.time()

# main_loc = 'C:/Users/RM/Documents/_ds_fairshares/Farmigo_Database/step_1_anonymization/Raw/'
main_loc = 'C:/Users/Fair Shares/Documents/_ds_fairshares/Farmigo_Database/step_1_anonymization/Raw/'

mem_del_folder = paste0(main_loc, 'member_deliveries/')
boxb_folder = paste0(main_loc, 'box_builder/') 
members_folder = paste0(main_loc, 'contacts/')
items_folder = paste0(main_loc, 'items/') 
backup_inv_folder = paste0(main_loc, 'backup_inventory_upload/') 
subscriptions_folder = paste0(main_loc, 'subscriptions/') 
aggr_del_folder = paste0(main_loc, 'aggregated_deliveries/') 

```

## Google Drive Data

### Connection

```{r}
# gs4_auth(email = "", cache = TRUE)
gs4_auth(cache = ".secrets", email = TRUE, use_oob = TRUE)

# sheets_auth()
# https://stackoverflow.com/questions/44980757/remote-server-authentication-to-read-googlesheets-from-r-script-not-using-servic/59910070#59910070
```

### Share Rotation

```{r}
start_time = Sys.time()


sr = range_read("1xs8TAMrSsJuL_gou4y0DBH3IkaTH0eBn_pdboCGWFTI"
                ,sheet = 'share_rotation'
                ,col_types = 'Dicicncc'
)

sr_wks = sr |> 
  select(fs_date = date, fs_date_string = date_string, season, season_week) |> 
  distinct() |> 
  mutate(fs_date_start = floor_date(fs_date, unit = 'week')
         ,fs_date_end = fs_date_start + 6)
```


## External Data Upload

### Member Information

```{r}
file_master_member = list.files(members_folder) |> 
  as_tibble() |> 
  rename(file = value) |>
  mutate(file_full = paste0(members_folder,file)
         , snapshot_date = ymd(str_sub(file,0,10))
         ) |> 
  arrange(snapshot_date) |> 
  rownames_to_column() |> 
  rename(rowname_itm = rowname)

## join files to FS dates
file_master_member_fs = file_master_member |> 
  full_join(sr_wks, by = character()) |> 
  filter(snapshot_date >= fs_date_start & snapshot_date <= fs_date_end) |> 
  group_by(fs_date) |> 
  arrange(fs_date, desc(snapshot_date)) |> 
  mutate(file_rank = rank(desc(snapshot_date))) |> 
  ungroup() |> 
  filter(file_rank == 1) |> 
  select(-file_rank)

## error check to make sure only one file selected
stopifnot((file_master_member_fs |> 
             group_by(fs_date) |> 
             summarise(n = n()) |> 
             filter(n > 1) |> 
             pull()
            ) > 0 )

## takes a bit to run so best to leave as separate table to test/run next few transformations
farmigo_member_orig = file_master_member_fs |> 
  left_join(file_master_member |> 
              pull(file_full) |> 
              map_df(~read_csv(.x), .id = "rowname_itm") |> 
              rename_all(tolower) |> 
              select_all(list(~gsub(" ", "_", .)))
            , by = c('rowname_itm')
  )

farmigo_member_history = farmigo_member_orig |> 
  select(date = fs_date, season, season_week, snapshot_date, member_id, route, pickup_site)

# head(farmigo_member_orig)
```

### Box Builder (Weekly Shares Items)

LOTS TO FIGURE OUT IN THIS FILE TO GET IT NORMALIZED. THIS FILE MAY BE TEDIOUS TO CLEAN

```{r eval=FALSE, include=FALSE}
file_boxbuilder = list.files(boxb_folder) |> 
  as_tibble() |> 
  rename(file = value) |>
  mutate(file_full = paste0(boxb_folder,file)
         , first_dash = str_locate(file,'-')[1]
         , start_date = mdy(str_sub(file, first_dash+1, first_dash+11))
         , end_date = mdy(str_sub(file, first_dash+12, first_dash+23))
         , snapshot_date = ymd(str_sub(file,0,10))
         ) |> 
  arrange(snapshot_date) |> 
  rownames_to_column() |> 
  rename(rowname_del = rowname)

file_boxbuilder_fs = file_boxbuilder |> 
  full_join(sr_wks, by = character()) |> 
  filter(fs_date >= start_date & fs_date <= end_date) |> 
  group_by(fs_date) |> 
  arrange(fs_date, desc(snapshot_date)) |> 
  mutate(file_rank = rank(desc(snapshot_date))) |> 
  ungroup()

## error check (cannot have more than one file week associated with a week of data)
stopifnot((file_boxbuilder_fs |> 
             filter(file_rank == 1) |> 
             group_by(fs_date) |> 
             summarise(n = n()) |> 
             filter(n > 1) |> 
             pull()
            ) > 0 ) 

## ISSUE IN THIS STEP. COLUMN HEADERS AREN'T THE SAME IN ALL FILES. I NEED TO TRANSFORM THE FILE BEFORE JOINING IT TO THE ORIGINAL FILE.
## takes a bit to run so best to leave as separate table to test/run next few transformations
# test = file_boxbuilder |> 
#   filter(rowname_del %in% c(4)) |> 
#   pull(file_full) |> 
#   map_df(~read_csv(.x), .id = "rowname_del")
# 
# 
# farmigo_boxbuilder_orig = file_boxbuilder |> 
#   left_join(file_boxbuilder |> 
#               pull(file_full) |> 
#               map_df(~read_csv(.x), .id = "rowname_del")
#               ## DATA TRANSFORMATION LOGIC NEEDS TO BE ADDED HERE!!!
#             , by = c('rowname_del')
#   )
# 
# 
# 
# farmigo_boxbuilder = farmigo_boxbuilder_orig |> 
#   mutate(delivery_date = ymd(delivery_date) ) |> 
#   inner_join(file_boxbuilder_fs |> 
#                filter(file_rank == 1) |> 
#                select(rowname_del, snapshot_date
#                       , fs_date, fs_date_string, fs_date_start, fs_date_end)
#              , by = c('rowname_del','snapshot_date')
#   ) |> 
#   relocate(fs_date, fs_date_string, fs_date_start, fs_date_end) |> 
#   filter(delivery_date >= fs_date_start & delivery_date <= fs_date_end) |> 
#   select(-file, -file_full, -start_date, -end_date)
# 
# ## error check to make sure a week is not being duplicated by multiple files
# stopifnot((farmigo_boxbuilder |> 
#              select(fs_date, rowname_del) |> 
#              distinct() |> 
#              group_by(fs_date) |> 
#              summarise(n = n()) |> 
#              filter(n > 1) |> 
#              pull()
#             ) > 0 )
# 
# ## error check to make sure a delivery date is not being duplicated by multiple files
# stopifnot((farmigo_boxbuilder |> 
#              select(delivery_date, rowname_del) |> 
#              distinct() |> 
#              group_by(delivery_date) |> 
#              summarise(n = n()) |> 
#              filter(n > 1) |> 
#              pull()
#             ) > 0 )
```

### Member Deliveries (aka Sales)

```{r}
file_master_memb_del = list.files(mem_del_folder) |> 
  as_tibble() |> 
  rename(file = value) |>
  mutate(file_full = paste0(mem_del_folder,file)
         , first_dash = str_locate(file,'-')[1]
         , start_date = mdy(str_sub(file, first_dash+1, first_dash+11))
         , to_find = str_locate(file,'to-')[1]
         , end_date = mdy(str_sub(file, to_find+3, to_find+12))
         , snapshot_date = ymd(str_sub(file,0,10))
         ) |> 
  select(-first_dash, -to_find) |> 
  filter(start_date < end_date) |> #test to make sure no odd files
  rownames_to_column() |> 
  rename(rowname_del = rowname)

file_master_allweeks_memb_del = file_master_memb_del |> 
  full_join(sr_wks, by = character()) |> 
  filter(fs_date >= start_date & fs_date <= end_date) |> 
  group_by(fs_date) |> 
  arrange(fs_date, desc(snapshot_date)) |> 
  mutate(file_rank = rank(desc(snapshot_date))) |> 
  ungroup()

## error check (cannot have more than one file week associated with a week of data)
stopifnot((file_master_allweeks_memb_del |> 
             filter(file_rank == 1) |> 
             group_by(fs_date) |> 
             summarise(n = n()) |> 
             filter(n > 1) |> 
             pull()
            ) > 0 ) 

## takes a bit to run so best to leave as separate table to test/run next few transformations
farmigo_memb_del_orig = file_master_memb_del |> 
  left_join(file_master_memb_del |> 
              pull(file_full) |> 
              map_df(~read_csv(.x), .id = "rowname_del")
            , by = c('rowname_del')
  )

farmigo_memb_del_orig_join = farmigo_memb_del_orig |> 
  mutate(delivery_date = ymd(delivery_date) ) |> 
  inner_join(file_master_allweeks_memb_del |> 
               filter(file_rank == 1) |> 
               select(rowname_del, snapshot_date
                      , fs_date, fs_date_string, fs_date_start, fs_date_end, season, season_week)
             , by = c('rowname_del','snapshot_date')
  ) |> 
  relocate(fs_date, fs_date_string, fs_date_start, fs_date_end) |> 
  filter(delivery_date >= fs_date_start & delivery_date <= fs_date_end)


## error check to make sure a week is not being duplicated by multiple files
stopifnot((farmigo_memb_del_orig_join |> 
             select(fs_date, rowname_del) |> 
             distinct() |> 
             group_by(fs_date) |> 
             summarise(n = n()) |> 
             filter(n > 1) |> 
             pull()
            ) > 0 )

## error check to make sure a delivery date is not being duplicated by multiple files
stopifnot((farmigo_memb_del_orig_join |> 
             select(delivery_date, rowname_del) |> 
             distinct() |> 
             group_by(delivery_date) |> 
             summarise(n = n()) |> 
             filter(n > 1) |> 
             pull()
            ) > 0 )

farmigo_memb_del = farmigo_memb_del_orig_join |> 
  select(date = fs_date, season, season_week, snapshot_date, delivery_date, member_id, item_unit, sales, price)
  # select(-file, -file_full, -start_date, -end_date) 


# farmigo_del_cln_check = farmigo_memb_del |> 
#   filter(member_id %in% c(99,467)) |> 
#   arrange(fs_date, member_id, item_unit)

```

### All Items

#### Master All Item File (Farmigo)

```{r}

file_master_itm = list.files(items_folder) |> 
  as_tibble() |> 
  rename(file = value) |>
  mutate(file_full = paste0(items_folder,file)
         , snapshot_date = mdy(str_sub(file,
                                     str_locate(file
                                                ,'-')[2]+1
                                     , -5)
                             )
         , inv_usable = ifelse(str_sub(file,0,1)=='x', 'N', 'Y')
         ) |> 
  arrange(snapshot_date) |> 
  rownames_to_column() |> 
  rename(rowname_itm = rowname)

## join files to FS dates
file_master_itm_fs = file_master_itm |> 
  full_join(sr_wks
            , by = character()) |> 
  filter(snapshot_date >= fs_date_start & snapshot_date <= fs_date_end) |> 
  group_by(fs_date) |> 
  arrange(fs_date, desc(snapshot_date), desc(inv_usable)) |> 
  mutate(file_rank = rank(desc(snapshot_date))) |> 
  ungroup() |> 
  filter(file_rank == 1) |> 
  select(-file_rank)

## error check to make sure only one file selected
stopifnot((file_master_itm_fs |> 
             group_by(fs_date) |> 
             summarise(n = n()) |> 
             filter(n > 1) |> 
             pull()
            ) > 0 )

## takes a bit to run so best to leave as separate table to test/run next few transformations
farmigo_itm_orig = file_master_itm_fs |> 
  left_join(file_master_itm |> 
              pull(file_full) |> 
              map_df(~read_csv(.x), .id = "rowname_itm") |> 
              rename_all(tolower) |> 
              select_all(list(~gsub(" ", "_", .)))
            , by = c('rowname_itm')
  )

head(farmigo_itm_orig)
```

#### Backup Inventory Upload (from Food List)

This logic was created in case we forgot to get a Farmigo copy of the All Item data set. This process is not used anymore. 

```{r}
file_master_upld = list.files(backup_inv_folder) |> 
  as_tibble() |> 
  rename(file = value) |>
  mutate(file_full = paste0(backup_inv_folder,file)
         , snapshot_date = mdy(str_sub(file,
                                     str_locate(file
                                                ,'__')[2]+2
                                     , -5)
                             )
         ) |> 
  arrange(snapshot_date) |> 
  rownames_to_column() |> 
  rename(rowname_upld = rowname)

## join files to FS dates
file_master_upld_fs = file_master_upld |> 
  full_join(sr_wks
            , by = character()) |> 
  filter(snapshot_date >= fs_date_start & snapshot_date <= fs_date_end) |> 
  group_by(fs_date) |> 
  arrange(fs_date, desc(snapshot_date)) |> 
  mutate(file_rank = rank(desc(snapshot_date))) |> 
  ungroup() |> 
  filter(file_rank == 1) |> 
  select(-file_rank)

## error check to make sure only one file selected
stopifnot((file_master_upld_fs |> 
             group_by(fs_date) |> 
             summarise(n = n()) |> 
             filter(n > 1) |> 
             pull()
            ) > 0 )

## takes a bit to run so best to leave as separate table to test/run next few transformations
farmigo_upld_orig = file_master_upld_fs |> 
  left_join(file_master_upld |> 
              pull(file_full) |> 
              map_df(~read_csv(.x), .id = "rowname_upld") |> 
              rename_all(tolower) |> 
              select_all(list(~gsub(" ", "_", .)))
            , by = c('rowname_upld')
  )

head(farmigo_upld_orig)
```

#### All Item / Backup Inventory Upload Combined

```{r}
file_master_inv = sr_wks |> 
  filter(fs_date <= Sys.Date()) |> # may need to change this date filter in future
  left_join(file_master_itm_fs |> 
              select(fs_date, itm_inv = inv_usable, rowname_itm)
            , by = c('fs_date')
            ) |> 
  left_join(file_master_upld_fs |>
              mutate(upld = 'Y') |> 
              select(fs_date, upld, rowname_upld)
            , by = c('fs_date')
            ) |> 
  replace_na(list(itm_inv = ' ', upld = ' ')) |> 
  mutate(file_status = ifelse(itm_inv == 'Y', 'farmigo file', 
                              ifelse(upld == 'Y', 'upload file', 'none'))
         )

### Combine three data sets:
### 1. Weeks with no inventory data (Upload OR farmigo file) - leaving out for now
  ### This adds extra data as we have to use newer files to assume item information
  ### will look at last to see if truly needed

### 2. Weeks with no inventory BUT with farmigo file
inv_all_itm_no_inv = file_master_inv |> 
  filter(file_status == 'none' & !is.na(rowname_itm)) |> 
  left_join(farmigo_itm_orig |> 
              select(-fs_date, -fs_date_string, -season, -season_week, -fs_date_start, -fs_date_end)
            , by = c('rowname_itm')
            )

### 3. Weeks with item inventory (all items)
inv_all_itm = file_master_inv |> 
  filter(file_status == 'farmigo file') |> 
  left_join(farmigo_itm_orig |> 
              select(-fs_date, -fs_date_string, -season, -season_week, -fs_date_start, -fs_date_end)
            , by = c('rowname_itm')
            )

### 4. Weeks with upload inventory (all items)
  ### Items where Item and Upload file BOTH present in the week
inv_upld_all_itm = file_master_inv |> 
  filter(file_status == 'upload file') |> 
  left_join(farmigo_itm_orig |> 
              select(-fs_date, -fs_date_string, -season, -season_week, -fs_date_start, -fs_date_end,
                     -file, -file_full, -snapshot_date)
            , by = c('rowname_itm')
            ) |> 
  left_join(farmigo_upld_orig |> 
              select(-fs_date, -fs_date_string, -season, -season_week, -fs_date_start, -fs_date_end, upload = available)
            , by = c('rowname_upld', 'item')
            ) 
  ### Items where ONLY Upload present, THEN combine with the table above

inv_upld = inv_upld_all_itm |> 
  filter(!fs_date %in% c(inv_upld_all_itm |> 
                                  select(fs_date, file_status) |> 
                                  distinct() |> 
                                  group_by(fs_date) |> 
                                  summarise(n = n()) |> 
                                  filter(n == 1) |> 
                                  select(fs_date) |> 
                                  pull()
                         )
         ) |> 
  union_all(file_master_inv |> 
        filter(file_status == 'upload file' & 
                 fs_date %in% c(inv_upld_all_itm |> 
                                  select(fs_date, file_status) |> 
                                  distinct() |> 
                                  group_by(fs_date) |> 
                                  summarise(n = n()) |> 
                                  filter(n == 1) |> 
                                  select(fs_date) |> 
                                  pull()
                   )
               ) |> 
        left_join(farmigo_upld_orig |> 
                    mutate(inv_usable = NA, avail = available) |> 
                    select(-fs_date, -fs_date_string, -season, -season_week, -fs_date_start, -fs_date_end, inv_usable, -available) |> 
                    relocate(item, .after = last_col())
                  , by = c('rowname_upld')
                  ) |> 
    mutate(category = NA, additional_categories = NA, available = avail, price = NA, status = NA, producer = NA, title_spanish = NA,
           unit_quantity = NA, unit = NA, is_credit = NA, pack_separately = NA, max_per_member = NA, image_url = NA,
           description = NA, link = NA, weight = NA, label = NA, upload = avail) |> 
    select(-avail)
  ) |> 
  union_all(inv_all_itm_no_inv |> 
              mutate(upload = available))

#### Combine into a single table
inv_comb = inv_all_itm |>
  mutate(upload = NA) |> 
  union_all(inv_upld) |> 
  mutate(start_inv = case_when(
    file_status == 'farmigo file' ~ available,
    file_status == 'upload file' ~ upload)
    ) |> 
  arrange(fs_date, item)

## Checking to make sure value look similar and data not being excluded
## >= 100 filter to get rid of items we assume we have an unlimited supply of
# inv_comb |> 
#   group_by(fs_date) |> 
#   summarise(n = n(),
#             start_inv = sum(ifelse(start_inv >= 100, 0, start_inv), na.rm=TRUE))
# 
# farmigo_upld_orig |> 
#   group_by(fs_date) |> 
#   summarise(n = n(),
#             start_inv = sum(ifelse(available >= 100, 0, available), na.rm=TRUE))
# 
# farmigo_itm_orig |> 
#   group_by(fs_date) |> 
#   summarise(n = n(),
#             start_inv = sum(ifelse(available >= 100, 0, available), na.rm=TRUE))


farmigo_all_item = inv_comb |> 
  select(date = fs_date, season, season_week, snapshot_date, file_status, item, category, start_available = start_inv,
         price, status, producer, unit_quantity, unit, max_per_member)
```


### All Subscriptions

```{r}
file_all_subscr = list.files(subscriptions_folder) |> 
  as_tibble() |> 
  rename(file = value) |>
  mutate(file_full = paste0(subscriptions_folder,file)
         , snapshot_date = mdy(str_replace_all(str_sub(file,19,28),'_','-'))
         ) |> 
  arrange(snapshot_date) |> 
  rownames_to_column() |> 
  rename(rowname_itm = rowname)

## join files to FS dates
file_all_subscr_fs = file_all_subscr |> 
  full_join(sr_wks, by = character()) |> 
  filter(snapshot_date >= fs_date_start & snapshot_date <= fs_date_end) |> 
  group_by(fs_date) |> 
  arrange(fs_date, desc(snapshot_date)) |> 
  mutate(file_rank = rank(desc(snapshot_date))) |> 
  ungroup() |> 
  filter(file_rank == 1) |> 
  select(-file_rank)

## error check to make sure only one file selected
stopifnot((file_all_subscr_fs |> 
             group_by(fs_date) |> 
             summarise(n = n()) |> 
             filter(n > 1) |> 
             pull()
            ) > 0 )

## takes a bit to run so best to leave as separate table to test/run next few transformations
farmigo_all_subscr_orig = file_all_subscr_fs |> 
  left_join(file_all_subscr |> 
              pull(file_full) |> 
              map_df(~read.csv(.x), .id = "rowname_itm") |> 
              rename_all(tolower) # |>
              # select_all(list(~gsub(".", "_", .)))
            , by = c('rowname_itm')
  )

farmigo_all_subscr_final = farmigo_all_subscr_orig |> 
  select(date = fs_date, season = season.x, season_week, snapshot_date
         , item, category, start_available = available
         , price, status, producer, unit_quantity = unit.quantity, unit, max_per_member = max.per.member
         , schedules)

# head(farmigo_all_subscr_final)

```

### Aggregated Delivery Data

```{r}
file_master_del_aggr = list.files(aggr_del_folder) |> 
  as_tibble() |> 
  rename(file = value) |>
  mutate(file_full = paste0(aggr_del_folder,file)
         , first_dash = str_locate(file,'-')[1]
         , start_date = mdy(str_sub(file, first_dash+1, first_dash+11))
         , to_find = str_locate(file,'to-')[1]
         , end_date = mdy(str_sub(file, to_find+3, to_find+12))
         , snapshot_date = mdy(str_sub(file,
                                     str_locate(file
                                                ,'__')[2]+2
                                     , -5)
                             )
         ) |> 
  select(-first_dash, - to_find) |> 
  filter(start_date < end_date) |> #test to make sure no odd files
  rownames_to_column() |> 
  rename(rowname_del = rowname)

file_master_allweeks_del_aggr = file_master_del_aggr |> 
  full_join(sr_wks, by = character()) |> 
  filter(fs_date >= start_date & fs_date <= end_date) |> 
  group_by(fs_date) |> 
  arrange(fs_date, desc(snapshot_date)) |> 
  mutate(file_rank = rank(desc(snapshot_date))) |> 
  ungroup()

## error check (cannot have more than one file week associated with a week of data)
stopifnot((file_master_allweeks_del_aggr |> 
             filter(file_rank == 1) |> 
             group_by(fs_date) |> 
             summarise(n = n()) |> 
             filter(n > 1) |> 
             pull()
            ) > 0 ) 

## takes a bit to run so best to leave as separate table to test/run next few transformations
farmigo_del_aggr_orig = file_master_del_aggr |> 
  left_join(file_master_del_aggr |> 
              pull(file_full) |> 
              map_df(~read_csv(.x), .id = "rowname_del")
            , by = c('rowname_del')
  )

farmigo_del_aggr = farmigo_del_aggr_orig |> 
  rename(delivery_date = 'Delivery Date') |> 
  filter(str_sub(delivery_date, 1, 5) == 'Total') |> 
  mutate(delivery_date = ymd(str_replace(delivery_date,'Total - ','')) ) |> 
  inner_join(file_master_allweeks_del_aggr |> 
               filter(file_rank == 1) |> 
               select(rowname_del, snapshot_date
                      , fs_date, fs_date_string, fs_date_start, fs_date_end)
             , by = c('rowname_del','snapshot_date')
  ) |> 
  relocate(fs_date, fs_date_string, fs_date_start, fs_date_end) |> 
  filter(delivery_date >= fs_date_start & delivery_date <= fs_date_end) |> 
  select(-file, -file_full, -start_date, -end_date)

## error check to make sure a week is not being duplicated by multiple files
stopifnot((farmigo_del_aggr |> 
             select(fs_date, rowname_del) |> 
             distinct() |> 
             group_by(fs_date) |> 
             summarise(n = n()) |> 
             filter(n > 1) |> 
             pull()
            ) > 0 )

## error check to make sure a delivery date is not being duplicated by multiple files
stopifnot((farmigo_del_aggr |> 
             select(delivery_date, rowname_del) |> 
             distinct() |> 
             group_by(delivery_date) |> 
             summarise(n = n()) |> 
             filter(n > 1) |> 
             pull()
            ) > 0 )

farmigo_del_aggr_cln = farmigo_del_aggr |> 
  pivot_longer(cols = -c(fs_date, fs_date_string, fs_date_start, fs_date_end
                         , rowname_del, snapshot_date, delivery_date)
               , names_to = "item_unit"
               , values_to = "sales") |> 
  filter(sales > 0) |> 
  mutate(extra_space = str_locate(item_unit,'  -')[,'end']
         ,extra_space = ifelse(is.na(extra_space), 0, extra_space)
         ,last_dash = map(str_locate_all(item_unit,' -'), max)
         ,item_unit = ifelse(extra_space == last_dash
                            ,paste0(str_sub(item_unit, 1, str_locate(item_unit,'  -')[,'end']-2)
                                    , str_sub(item_unit, str_locate(item_unit,'  -')[,'end'], -1)
                                    )
                            , item_unit)
         ) |> 
  select(-extra_space, -last_dash)

head(farmigo_del_aggr_cln)

farmigo_del_aggr_cln_wkly = farmigo_del_aggr_cln |> 
  group_by(fs_date, fs_date_string, fs_date_start, fs_date_end, item_unit) |> 
  summarize(sales = sum(sales)
            , rowname_del = paste0(unique(rowname_del), collapse = ",")
            , snapshot_date = paste0(unique(snapshot_date), collapse = ",")) |> 
  ungroup() |> 
  left_join(file_master_allweeks_del_aggr |> 
              filter(file_rank == 1) |> 
              select(fs_date) |> 
              mutate(del_file = TRUE)
            , by = c('fs_date')
            ) |> 
  left_join(sr_wks |> 
              select(fs_date, season, season_week)
            , by = ('fs_date')
            ) |> 
  select(date = fs_date, season, season_week, snapshot_date, item_unit, sales, del_file)

head(farmigo_del_aggr_cln_wkly |> arrange(item_unit, date))

```


## Google Sheet Data Limit Check

Google sheets has a cell limit of 10Million. I should build in checks to all large data sets, like this a check for row/column size, multiplied by each other, to show a percentage of that 10 Million limit. 

```{r}
# member history
cell_count_member_history = dim(farmigo_member_history)[1]*dim(farmigo_member_history)[2]

print(paste0('percentage filled to Google Sheets maximum cell count (Member History) ',(cell_count_member_history / 10000000)*100, '%'))

stopifnot(cell_count_member_history <= 10000000)

# boxbuilder
## to be built

# member deliveries
cell_count_memb_del = dim(farmigo_memb_del)[1]*dim(farmigo_memb_del)[2]

print(paste0('percentage filled to Google Sheets maximum cell count (Member Deliveries) ',(cell_count_memb_del / 10000000)*100, '%'))

stopifnot(cell_count_memb_del <= 10000000)

# all items
cell_count_all_item = dim(farmigo_all_item)[1]*dim(farmigo_all_item)[2]

print(paste0('percentage filled to Google Sheets maximum cell count (All Items) ',(cell_count_all_item / 10000000)*100, '%'))

stopifnot(cell_count_all_item <= 10000000)

# all subscriptions
cell_count_all_subscr = dim(farmigo_all_subscr_final)[1]*dim(farmigo_all_subscr_final)[2]

print(paste0('percentage filled to Google Sheets maximum cell count (All Subscriptions) ',(cell_count_all_subscr / 10000000)*100, '%'))

stopifnot(cell_count_all_subscr <= 10000000)

# aggregated delivery date
cell_count_del_aggr = dim(farmigo_del_aggr_cln_wkly)[1]*dim(farmigo_del_aggr_cln_wkly)[2]

print(paste0('percentage filled to Google Sheets maximum cell count (Delivery Aggregate) ',(cell_count_del_aggr / 10000000)*100, '%'))

stopifnot(cell_count_del_aggr <= 10000000)

```


## Upload Data Sets


### sheet id
```{r}
sheet_id_farmigo = "1W34-fLMAWPt5VvWb6kiVxWPikgn0Ly2e6JjhQIOmsF4"
```


### Upload

```{r}
# member history
sheet_write(farmigo_member_history, sheet_id_farmigo, sheet = "member_group_history")

# boxbuilder
## to be built

# member deliveries
sheet_write(farmigo_memb_del, sheet_id_farmigo, sheet = "sales_deliveries")

# all items
sheet_write(farmigo_all_item, sheet_id_farmigo, sheet = "all_items")

# all subscriptions
sheet_write(farmigo_all_subscr_final, sheet_id_farmigo, sheet = "all_subscriptions")

# aggregated delivery date
sheet_write(farmigo_del_aggr_cln_wkly, sheet_id_farmigo, sheet = "aggregated_deliveries")
```


```{r}
rm(sheet_id_farmigo)

end_time = Sys.time()

start_time
end_time
```

